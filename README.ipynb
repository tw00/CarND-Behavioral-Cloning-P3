{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# README (Behavioral Cloning Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue'>**NOTE TO REVIEWER:**</span>\n",
    "\n",
    "<span style='color:blue'>As suggested I implemented the nvidia architecture as well (see model.py, line 320-341). After training the nvidia model, the car is now driving around the first track without leaving it. I also included a video.mp4.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Project structure\n",
    "\n",
    "## 1. Project files\n",
    "\n",
    "My project includes the following files:\n",
    "* self_driving_car/data_generator.py \n",
    "* self_driving_car/model.py  \n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.hdf5 containing a trained convolution neural network \n",
    "\n",
    "The `data_generator.py` contains two classes `DataPreprocessor` and `DataGenerator`. `DataPreprocessor` preprocesses the data and saves precomputed images in a separate `IMG_preprocessed` folder. Also, it creates an easy to process list of all samples (`index.pkl`). `DataGenerator` provides data during training, it also takes care of data balancing and filtering.\n",
    "\n",
    "The model.py file contains a class `SDRegressionModel` for training and saving different convolution neural networks as well as input normalization. \n",
    "\n",
    "## 2. Running the model\n",
    "Using the Udacity provided simulator (beta) and my drive.py file, the car can be driven autonomously by executing \n",
    "\n",
    "```sh\n",
    "python drive.py model.hdf5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model Architecture and Training Strategy\n",
    "\n",
    "## 1. Finding an appropriate model architecture\n",
    "\n",
    "I archived the best results with `model_nvidia`. This model consists of three convolutional layers with 4x4 and 8x8 filter sizes and depths 16 and three fully connected layers (see `self_driving_car/model.py`, `model_nvidia`). The architecture is shown in the following image:\n",
    "\n",
    "![alt text](./doc/images/model_nvidia.png)\n",
    "\n",
    "The model includes LeakyReLU layers to introduce nonlinearity (model.py lines 251-263).\n",
    "The data is normalized in the model using `SDRegressionModel.normalize` (model.py lines 66).\n",
    "\n",
    "## 2. Attempts to reduce overfitting in the model\n",
    "\n",
    "The model contains dropout layers in order to reduce overfitting (model.py lines 259-262). \n",
    "\n",
    "The model was trained and validated on different data sets to ensure that the model was not overfitting (code line 10-16). Training data is provided using a generator through the method\n",
    "`DataGenerator.get_batch_generator` (`self_driving_car/data_generator.py`),\n",
    "validation data is provided by the method\n",
    "`DataGenerator.get_valid_data` (`self_driving_car/data_generator.py`).\n",
    "\n",
    "## 3. Model parameter tuning\n",
    "\n",
    "The model used an Nesterov Adam (nadam) optimizer, so the learning rate was not tuned automatically (model.py line 25). I tried different optimizers adam and nadam both gave good results.\n",
    "\n",
    "## 4. Appropriate training data\n",
    "\n",
    "For training data, I used data provided by Udacity as well as a few own laps. I also drove the track backward.\n",
    "I also created numerous recordings of critical corners. This results in the following datasets:\n",
    "\n",
    "```\n",
    "dataset1_udacity      dataset4_beta_sim        dataset7_curve2B  dataset8_curve3C\n",
    "dataset2_twe_one_lap  dataset5_beta_backwards  dataset8_curve3A  dataset8_curve3D\n",
    "dataset3_ssz_one_lap  dataset6_curve2A         dataset8_curve3B\n",
    "```\n",
    "\n",
    "All training samples are kept in a table, that looks like (index.pkl):\n",
    "\n",
    "![alt text](./doc/table.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model Architecture and Training Strategy\n",
    "\n",
    "## 1. Solution Design Approach\n",
    "\n",
    "My overall strategy includes the following steps:\n",
    "\n",
    "1. Generating more data\n",
    "2. Augmenting the data\n",
    "3. Filtering and balancing the training data\n",
    "4. Train a few epochs\n",
    "5. Calculate accuracy using validation data\n",
    "6. Testing the model on track 1\n",
    "7. Go back to step 1 further improving the best model\n",
    "(all steps are documented in `CarND_Behavioral_Cloning_Training_Part5.ipynb`)\n",
    "\n",
    "I repeated this process for different architectures:\n",
    "\n",
    "* The CommaAI architecture\n",
    "* A modified CommaAI architecture with cropping and normalization\n",
    "* A simple architecture with a low neuron count\n",
    "* A modified 2nd simple architecture with more convolutional filters\n",
    "* A modified 3rd simple architecture with more fully connected neurons\n",
    "* A modified 4th simple architecture more tunable parameters\n",
    "* The NVIDIA architecture\n",
    "\n",
    "The reason why I created different models, was that I believed that I was not able to get a further improvement because the model complexity was too low so the model is not able to capture the complexity of the given task.\n",
    "\n",
    "I implemented different methods that allow me to filter and balance training data. As it turns out that both steps are crucial for successful training. I, therefore, implemented the following methods for the `DataGenerator` class. \n",
    "\n",
    "* `add_dataset(self, dataset, basepath = '/mnt/data/')`\n",
    "Allows to add (preprocessed) datasets, different datasets can be combined\n",
    "\n",
    "* `filter_data_not_moving(self, not_moving_threshold = 10.)`\n",
    "Removes frames at which the car is not moving or moving very slow\n",
    "\n",
    "* `filter_data_low_steering(self, low_steering_threshold = 0.025, low_steering_remove_prop = 0.5)`\n",
    "Removes samples with low steering angle with a given propability\n",
    "\n",
    "* `smooth_steering(self, cam = 'C', window = 4)`\n",
    "Smooths the steering angle\n",
    "\n",
    "* `shuffle(self)`\n",
    "Shuffle training data\n",
    "\n",
    "* `correct_camera_steering(self, offset = 0.0)`\n",
    "Correct the camera steering angle for left/right camera images\n",
    "\n",
    "* `activate_mod(self, mod, dset='train')`\n",
    "Activates precomputed modifications (image augmentations)\n",
    "\n",
    "* `deactivate_cam(self, cam)`\n",
    "Deactivates samples that were taken with a certain camera\n",
    "\n",
    "* `split(self, valid_size=0.1)`\n",
    "Splits data into training and validation dataset\n",
    "\n",
    "Executing these methods in the correct order is important.\n",
    "\n",
    "For data augmentation I implemented different filters:\n",
    "\n",
    "* `DataPreprocessor.mod_identity` --> no modification\n",
    "* `DataPreprocessor.mod_lighting` --> randomly modifies the lighting\n",
    "* `DataPreprocessor.mod_blur`     --> blurs images\n",
    "* `DataPreprocessor.mod_flip`     --> flips images\n",
    "* `DataPreprocessor.mod_shadow`   --> randomly adds shadows\n",
    "\n",
    "I always tried to keep the dataset balance, which means that it contains equal amounts of left and right steering samples.\n",
    "\n",
    "The final step was to run the simulator to see how well the car was driving around track one.\n",
    "\n",
    "\n",
    "## 2. Final Model Architecture\n",
    "\n",
    "The model architecture that gives the best results (model.py lines 245 - 268) looks like:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/255.-0.5, input_shape=(66,200,3)) )\n",
    "model.add(Convolution2D(24, 5, 5, border_mode=\"same\", subsample=(2,2), activation=\"elu\"))\n",
    "model.add(SpatialDropout2D(0.2))\n",
    "model.add(Convolution2D(36, 5, 5, border_mode=\"same\", subsample=(2,2), activation=\"elu\"))\n",
    "model.add(SpatialDropout2D(0.2))\n",
    "model.add(Convolution2D(48, 5, 5, border_mode=\"valid\", subsample=(2,2), activation=\"elu\"))\n",
    "model.add(SpatialDropout2D(0.2))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "model.add(SpatialDropout2D(0.2))\n",
    "model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "model.add(SpatialDropout2D(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation=\"elu\"))\n",
    "model.add(Dense(50, activation=\"elu\"))\n",
    "model.add(Dense(10, activation=\"elu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))                                                        \n",
    "```\n",
    "\n",
    "The model consisted of a convolution neural network with the following layers and layer sizes:\n",
    "\n",
    "```\n",
    "____________________________________________________________________________________________________\n",
    "Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "====================================================================================================\n",
    "lambda_36 (Lambda)               (None, 66, 200, 3)    0           lambda_input_2[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_96 (Convolution2D) (None, 33, 100, 24)   1824        lambda_36[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "spatialdropout2d_96 (SpatialDrop (None, 33, 100, 24)   0           convolution2d_96[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_97 (Convolution2D) (None, 17, 50, 36)    21636       spatialdropout2d_96[0][0]        \n",
    "____________________________________________________________________________________________________\n",
    "spatialdropout2d_97 (SpatialDrop (None, 17, 50, 36)    0           convolution2d_97[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_98 (Convolution2D) (None, 7, 23, 48)     43248       spatialdropout2d_97[0][0]        \n",
    "____________________________________________________________________________________________________\n",
    "spatialdropout2d_98 (SpatialDrop (None, 7, 23, 48)     0           convolution2d_98[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_99 (Convolution2D) (None, 5, 21, 64)     27712       spatialdropout2d_98[0][0]        \n",
    "____________________________________________________________________________________________________\n",
    "spatialdropout2d_99 (SpatialDrop (None, 5, 21, 64)     0           convolution2d_99[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_100 (Convolution2D (None, 3, 19, 64)     36928       spatialdropout2d_99[0][0]        \n",
    "____________________________________________________________________________________________________\n",
    "spatialdropout2d_100 (SpatialDro (None, 3, 19, 64)     0           convolution2d_100[0][0]          \n",
    "____________________________________________________________________________________________________\n",
    "flatten_20 (Flatten)             (None, 3648)          0           spatialdropout2d_100[0][0]       \n",
    "____________________________________________________________________________________________________\n",
    "dropout_39 (Dropout)             (None, 3648)          0           flatten_20[0][0]                 \n",
    "____________________________________________________________________________________________________\n",
    "dense_77 (Dense)                 (None, 100)           364900      dropout_39[0][0]                 \n",
    "____________________________________________________________________________________________________\n",
    "dense_78 (Dense)                 (None, 50)            5050        dense_77[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "dense_79 (Dense)                 (None, 10)            510         dense_78[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "dropout_40 (Dropout)             (None, 10)            0           dense_79[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "dense_80 (Dense)                 (None, 1)             11          dropout_40[0][0]                 \n",
    "====================================================================================================\n",
    "Total params: 501,819\n",
    "Trainable params: 501,819\n",
    "Non-trainable params: 0\n",
    "____________________________________________________________________________________________________\n",
    "```\n",
    "\n",
    "I tried different architecture and 160k trainable parameters should be enough since I already came up with good results with only 18k parameters.\n",
    "\n",
    "\n",
    "## 3. Creation of the Training Set & Training Process\n",
    "\n",
    "To capture good driving behavior, I first recorded different laps on track one using center lane driving.\n",
    "Fist the images are resized and augmented, which looks like (here blurring and shadow):\n",
    "\n",
    "![alt text](./doc/blur.png)\n",
    "![alt text](./doc/shadow.png)\n",
    "\n",
    "For most model architecture the image is cropped, which gives an image of the relevant area:\n",
    "\n",
    "![alt text](./doc/cropping.png)\n",
    "\n",
    "After applying image augmentation and filtering samples, the training data looks well balanced.\n",
    "\n",
    "![alt text](./doc/table.png)\n",
    "\n",
    "This is the data that I used for the first 14 training epochs:\n",
    "\n",
    "![alt text](./doc/data1.png)\n",
    "\n",
    "The initial model I then train with more data:\n",
    "\n",
    "![alt text](./doc/data2.png)\n",
    "\n",
    "For the initial training both training error and validation error decrease:\n",
    "\n",
    "![alt text](./doc/loss1.png)\n",
    "\n",
    "Later no big change in the validation error is seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "During this project I tried different things to get a better model:\n",
    "\n",
    "* getting more training data\n",
    "* getting more training data of critical curves\n",
    "* increase model complexity\n",
    "* decrease model complexity\n",
    "* implement methods to prevent overfitting\n",
    "* train for more epochs (I trained for +4h an AWS)\n",
    "* augment data\n",
    "* removing low steering\n",
    "* cropping irrelevant data\n",
    "* different steering angle correction for left/right camera\n",
    "* initialization, different optimizers, different activation functions\n",
    "\n",
    "In order to get further improvement more augmentation and data from different tracks could be used. This could produce a more general model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
