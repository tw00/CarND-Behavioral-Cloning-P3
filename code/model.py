# Build a model
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Lambda, ELU
from keras.layers.core import Activation
from keras.layers.convolutional import Convolution2D
from keras.layers.pooling import MaxPooling2D
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from keras.callbacks import TensorBoard
#from keras import callbacks

def preprocess_img(img, fromFile = False, fromDriver = True):
    IMG_W = 128
    IMG_H = 64

    #img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
    #img = img.convert('RGB')
    img = np.array(img)
    #print(img);
    #print(img.shape);
    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV does not use RGB, it uses BGR
    img = cv2.resize(img, (IMG_W, IMG_H))

    img = img.astype(float)/255.0
    img = yuv_colorspace.rgb2yuv(img) # convert to YUV colorspace
    img[:,:,0] = img[:,:,0] - 0.5; # remove mean
    return img

class RegressionModel():

	def save_model_architecture(self, filename="model.json"):
		model = self.model
		with open(filename, "w") as f:
			json.dump(model.to_json(), f)

	def model_commaAI_modified():
		ch, row, col = 3, 64, 128  # camera format
		use_dropout = True

		model = Sequential()
		with tf.name_scope('conv2D_1'):
			model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode="same", input_shape=(row, col, ch)))
		model.add(ELU())
		with tf.name_scope('conv2D_2'):
			model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode="same"))
		model.add(ELU())
		with tf.name_scope('conv2D_3'):
			model.add(Convolution2D(64, 5, 5, subsample=(5, 5), border_mode="same"))
		model.add(Flatten())
		if use_dropout: # NEU
			model.add(Dropout(.2)) # NEU
		model.add(ELU())
		model.add(Dense(512))
		if use_dropout:
			model.add(Dropout(.5))
		model.add(ELU())
		model.add(Dense(20))
		model.add(ELU())
		model.add(Dense(1))

		# TODO: LR = 0.0001
		model.compile(optimizer="adam", loss="mse", metrics=['accuracy'])
		# optimizer = Adam(lr=learning_rate)
		# model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])
		# model.summary()

		return model

	def train_simple(X_train_norm, y_train_norm):
		# Working with large datasets like Imagenet #68
		# https://github.com/fchollet/keras/issues/68

		#model = model_nvidia([64,128,3])
		model = self.model
		history = model.fit(X_train_norm, y_train_norm, nb_epoch=100, validation_split=0)
		return history

	def train_generator():
		model = self.model
		model.load_weights("./output/model.h5")

		model.optimizer.lr.assign(0.0002);

		#checkpoint_callback = ModelCheckpoint("./output/model.hdf5", verbose=1, save_best_only=True);
		checkpoint_callback = ModelCheckpoint("./output/weights.{epoch:02d}-{val_loss:.4f}.hdf5", verbose=1);
		tensorboard_callback = TensorBoard(log_dir='./output/tb_log/');

#def checkpoint(filepattern="model.{epoch:02d}.h5"):
#return ModelCheckpoint(filepattern, save_weights_only=True)

		# Fit the model on the batches generated by datagen.flow().
		#                    samples_per_epoch=42*1500,
		with tf.name_scope('train'):
			history = model.fit_generator(generateBatchRandom(),
							samples_per_epoch=9*1500,
							nb_epoch=10,
							validation_data=(X_test_norm, y_test),
							callbacks=[checkpoint_callback, tensorboard_callback]);
            # TODO: Save trainig history

	def save(self, prefix):
		"""save model for future inspection and continuous training
		"""
		model_file = prefix + ".json"
		weight_file = prefix + ".h5"
		json.dump(self.model.to_json(), open(model_file, "w"))
		self.model.save_weights(weight_file)
		return self

	def restore(self, prefix):
		"""restore a saved model
		"""
		model_file = prefix + ".json"
		weight_file = prefix + ".h5"
		self.model = model_from_json(json.load(open(model_file)))
		self.model.load_weights(weight_file)
return self

if __name__ == '__main__':
    model = model_architecture()
	model.summary()
